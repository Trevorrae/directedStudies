{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, name, X, y, features, samplingModel=None):\n",
    "        self.name = name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.features = features\n",
    "        self.samplingModel = samplingModel #defaulted to None as it's not required\n",
    "        self.results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClass:\n",
    "    def __init__(self, name, actualModel):\n",
    "        self.name = name\n",
    "        self.actualModel = actualModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This global variable stores the names of the metric types that result from the classification models used in this project\n",
    "g_orderResults = ['Accuracy', 'Precision', 'Recall', 'F_Measure', 'Cross_Validation']\n",
    "\n",
    "# This global variable stores the number of metric types that are collected. Later, this global variable is very useful \n",
    "# for helping functions iterate through numpy array columns that contain the metric types from g_orderResults\n",
    "g_numResults = len(g_orderResults)\n",
    "\n",
    "# This global variable is a list that will store the classification models used in this project\n",
    "g_modelClasses = []\n",
    "\n",
    "# This global variable stores the different train/test splits that we will be performed on the datasets used in this project\n",
    "# A 0.2 split results in 20% of the dataset becoming the test dataset and 80% of the dataset becoming the training dataset.\n",
    "g_splits = np.array([0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "# This global variable stores the sampler models that will balance our datasets. g_underSampleModels will store all the under sampling\n",
    "# models. g_overSampleModels will store all the over sample models. g_comboSampleModels will store all the sample models that\n",
    "# use a combination of under and over sampling methods. \n",
    "g_underSampleModels = []\n",
    "g_overSampleModels = []\n",
    "g_comboSampleModels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre: A valid file path for a CSV file is passed in with the ignoreKey flag\n",
    "# Post: 1.) The CSV is read from the file (the first column (k1) if ignoreKey = true)\n",
    "#       2.) Each line (or instance) is read from disk. (k1, f1, f2, f3,...,fn,c)\n",
    "#       3.) All features ([k1][f1:fn]) are normalized and stored in the X array.\n",
    "#       4.) The last column is stored in the y array (representing the class)\n",
    "#       5.) A DatasetClass object is returned, representing the dataset.\n",
    "# @returns: An instance of a DatasetClass object.\n",
    "def load_csv_data(csv_file_path, ignoreKey):\n",
    "    #read the data into a panda data structure\n",
    "    data = pd.read_csv(csv_file_path)                   \n",
    "    \n",
    "    #rip out the X values (everything except the last column)\n",
    "    startIndex = 0\n",
    "    #ignore the first column (ie the key) if the ignoreKey flag is true\n",
    "    if ignoreKey:\n",
    "        startIndex = 1\n",
    "    rawX = data.iloc[:,startIndex:-1].values  # -1 <-- leave the last column for y\n",
    "    #normalize the X data\n",
    "    maxX = np.max(rawX, axis=0)\n",
    "    minX = np.min(rawX, axis=0)\n",
    "    avgX = np.mean(rawX, axis=0)\n",
    "    X = (rawX-minX)/(maxX-minX)\n",
    "\n",
    "    #Get the class \n",
    "    y = data.iloc[:,-1:].values\n",
    "    # check the shape of the class (single dimension matching the number of observations)\n",
    "    y = np.ravel(y)\n",
    "    # remove CSV from the filename, as we will use this as the dataset name\n",
    "    NameNoCSV = csv_file_path[:-4]\n",
    "    # stash everything in our custom DatasetClass\n",
    "    dsc = DatasetClass(NameNoCSV, X, y, data.columns[startIndex:-1])\n",
    "    \n",
    "    return dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
